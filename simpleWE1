import tensorflow as tf
from tensorflow import keras
from keras import layers
from keras.preprocessing.text import Tokenizer,text_to_word_sequence
from keras.layers import LSTM,Dense,Embedding
from keras.models import Sequential
import numpy as np
import string

#for versions
print("tf version:",tf.__version__)
print("keras version:",keras.__version__)
print("numpy:",np.__version__)

text = "African American newspapers in the 1930s faced many hardships. For instance, knowing that buyers of African American papers also bought general-circulation papers, advertisers of consumer products often ignored African American publications. Advertisers’ discrimination did free the African American press from advertiser domination. Editors could print politically charged material more readily than could the large national dailies, which depended on advertisers’ ideological approval to secure revenues. Unfortunately, it also made the selling price of Black papers much higher than that of general-circulation dailies. Often as much as two-thirds of publication costs had to come from subscribers orsubsidies from community politicians and other interest groups.And despite their editorial freedom, African American publishers often felt comeplled to print a disproportionate amount of sensationalism, sports, and society news to boost circulation."
txt = "".join(v for v in text if v not in string.punctuation).lower()
txt = txt.encode("utf8").decode("ascii","ignore")

tokenizer = Tokenizer(num_words=200)

def get_sequence_of_tokens(txt):
  tokenizer = Tokenizer(num_words=200)
  tokenizer.fit_on_texts(txt.split(" "))
  tokens = text_to_word_sequence(txt)
  total_words = len(tokenizer.word_index) + 1

  print("no_of_tokens:",len(tokens),"\n","avaliable tokens:",len(set(tokens)))
  print("token and tokenids:",tokenizer.word_index)
  print(len(tokenizer.word_index))
  get_sequence_of_tokens(txt)
  max_sequence_len = 131
total_words = 119

def create_model(max_sequence_len, total_words):
    input_len = max_sequence_len - 1
    model = Sequential()
    
    # Add Input Embedding Layer
    model.add(Embedding(total_words,10, input_length=input_len))
    
    # Add Hidden Layer 1 - LSTM Layer
    model.add(LSTM(10))

    # Add Output Layer
    model.add(Dense(total_words, activation='softmax'))

    model.compile(loss='categorical_crossentropy', optimizer='adam')
    
    return model
    from keras.utils import pad_sequences

def generate_text(seed_text,next_words, model, max_sequence_len):
  for i in range(next_words):
    token_list = tokenizer.texts_to_sequences(["Of African American newspapers"])
    token_list = pad_sequences([token_list],padding='pre')
    #predicted = model.predict_classes(token_list, verbose=0)

    output_word = ""
    for word,index in tokenizer.word_index.items():
      if index == predicted:
        output_Word = word
        break

      seed_text += " "+output_word
  return seed_text.title()
  generate_text(seed_text="circulation",next_words=10,model=model,max_sequence_len=5)
